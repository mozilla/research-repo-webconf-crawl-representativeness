# The representativeness of automated Web crawls as a surrogate for human browsing - companion repository

This repository contains or links to all assets relevant to the WWW'20 paper: [The representative of automated Web crawls as a surrogate for human browsing](https://dl.acm.org/doi/abs/10.1145/3366423.3380104). All listed assets will be made publicly available pending internal privacy/trust audit processes required prior to data release. For specific inquiries pertaining to data access and collaborations on privacy enhancing technologies research please reach out to the corresponding authors listed on the manuscript.

* Lists used for crawls - under lists directory
* Trexa repo - https://github.com/mozilla/trexa
* Crawl preparation - pre crawl and depth crawl code - https://github.com/mozilla/crawl-prep
* Crawl database - [Google Doc](https://docs.google.com/spreadsheets/d/1HlocB39Ujaw2JH4Nm_0lXFqQ6GcQjJ7ONHHLFq-NReI/)
* Crawl downloads - coming (est Apr 2020)
* Alternate orchestration repo - https://github.com/birdsarah/faust-selenium
* [List comparison analysis](./list-comparison/top-site-list-comparison.ipynb)
* DP-protected top-level domain visit counts for opt-in human users (est Apr 30, 2020)
